{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6LAQ_hFRP20y"
      },
      "source": [
        "**1. LLMChain**\n",
        "\n",
        "The LLMChain is the most basic and common chain. It combines a PromptTemplate and an LLM. It takes an input, formats it with the prompt template, passes it to the LLM, and returns the LLM's output.\n",
        "\n",
        "Structure: Input -> PromptTemplate -> LLM -> Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "t8ZPp9WWPw_s"
      },
      "outputs": [],
      "source": [
        "from langchain_openai import OpenAI\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UuyWq5D1QKHq"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "os.environ['OPENAI_API_KEY'] = 'sk-xxxxxxxxxxxxxxxxx'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "3MlRtwnOQDyx"
      },
      "outputs": [],
      "source": [
        "# Initialize the LLM\n",
        "llm = OpenAI(temperature=0.7)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "wl3BNkPSQD2B"
      },
      "outputs": [],
      "source": [
        "# Create a prompt template\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"name\", \"mood\"],\n",
        "    template=\"Write a short greeting for someone named {name} who is feeling {mood}.\"\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WR4-v5zgQD5A",
        "outputId": "bd7fbde1-2307-46d8-b2b8-5f84f848ab14"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-3487607905.py:2: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  greeting_chain = LLMChain(llm=llm, prompt=prompt)\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Create the chain\n",
        "greeting_chain = LLMChain(llm=llm, prompt=prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k272fIz4QD8N",
        "outputId": "5d160678-7ecb-49e7-8b26-b6694708559e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\"Hello Alice, it's wonderful to see you radiating with joy! Keep spreading that positive energy and have a fantastic day!\"\n"
          ]
        }
      ],
      "source": [
        "\n",
        "# Run the chain\n",
        "result = greeting_chain.invoke({\"name\": \"Alice\", \"mood\": \"joyful\"})\n",
        "print(result['text'])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "HpxX8IUCRaFD",
        "outputId": "3e872061-1128-4e4a-c1e9-1bd37d7f7bb0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "A high-tech wearable device designed to track and optimize athletic performance and fitness levels.\n"
          ]
        }
      ],
      "source": [
        "# Product Description Generator:\n",
        "\n",
        "prompt = PromptTemplate(\n",
        "    input_variables=[\"product\", \"audience\"],\n",
        "    template=\"Write a one-sentence description of {product} for {audience}.\"\n",
        ")\n",
        "chain = LLMChain(llm=llm, prompt=prompt)\n",
        "result = chain.invoke({\"product\": \"a smartwatch\", \"audience\": \"athletes\"})\n",
        "\n",
        "print(result['text'])\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zcZClarxQD_G",
        "outputId": "b2fb7fd9-c087-46b6-e7e9-5b62df7bc231"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/tmp/ipython-input-2138360038.py:27: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  print(chain.run(description))\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1. TechAutomate Solutions - Bangalore, Karnataka\n",
            "2. InnovateSoft Automation - Pune, Maharashtra\n",
            "3. CodeCraft Automation - Hyderabad, Telangana\n",
            "4. SmartTech Automations - Gurgaon, Haryana\n",
            "5. AutomateIT Technologies - Chennai, Tamil Nadu\n"
          ]
        }
      ],
      "source": [
        "#Example\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "from langchain import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "llm = ChatOpenAI()\n",
        "\n",
        "ITfirm_template = \"\"\"\n",
        "I want you to act as a consultant for a new IT firm.\n",
        "Return a list of firm names and a locality which is suitable for such a firm in India.\n",
        "The name should be catchy and the locality should yield profit.\n",
        "What are some good names and respective area for IT firm that does {firm_description}\n",
        "\"\"\"\n",
        "\n",
        "prompt_template = PromptTemplate(\n",
        "    input_variables=[\"firm_description\"],\n",
        "    template=ITfirm_template,\n",
        ")\n",
        "\n",
        "description = \"It is software dev firm specifically focusing on automation software\"\n",
        "\n",
        "prompt_template.format(firm_description=description)\n",
        "\n",
        "chain = LLMChain(llm=llm, prompt=prompt_template)\n",
        "\n",
        "print(chain.run(description))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z0GVWl2XScjp"
      },
      "source": [
        "# **Sequential Chains**\n",
        "\n",
        "A SequentialChain is used to execute multiple chains in a predefined order, passing the output of one chain as input to the next. This is the formal, preferred way to create multi-step workflows .\n",
        "\n",
        "There are two main types:\n",
        "\n",
        "SimpleSequentialChain: For chains with a single input and single output.\n",
        "\n",
        "SequentialChain: For chains with multiple inputs and/or multiple outputs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "metadata": {
        "id": "Y9vilGPtQEBn"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SimpleSequentialChain\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "rLmaTnZE_7zN"
      },
      "outputs": [],
      "source": [
        "llm = ChatOpenAI(model_name='gpt-4o-mini')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 18,
      "metadata": {
        "id": "urhTbhXMStzQ"
      },
      "outputs": [],
      "source": [
        "\n",
        "# First Chain: Translates English to French\n",
        "translation_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Translate this English text to French: {text}\"\n",
        ")\n",
        "translate_chain = LLMChain(llm=llm, prompt=translation_prompt)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "metadata": {
        "id": "aAFOFO-ISwJ5"
      },
      "outputs": [],
      "source": [
        "# Second Chain: Summarizes the translated text\n",
        "summary_prompt = PromptTemplate(\n",
        "    input_variables=[\"text\"],\n",
        "    template=\"Summarize this French text in one sentence: {text}\"\n",
        ")\n",
        "summarize_chain = LLMChain(llm=llm, prompt=summary_prompt)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "id": "tLBTt77hSxlf"
      },
      "outputs": [],
      "source": [
        "# Combine them\n",
        "overall_chain = SimpleSequentialChain(chains=[translate_chain, summarize_chain], verbose=True) #, verbose=True\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 22,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "STE_vDy_AJcN",
        "outputId": "988a9e33-7a6f-48c9-9801-ad7d7ff7a98f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SimpleSequentialChain chain...\u001b[0m\n",
            "\u001b[36;1m\u001b[1;3mUn long article sur l'histoire d'Internet...\u001b[0m\n",
            "\u001b[33;1m\u001b[1;3mUn article détaillé retrace l'évolution et les événements marquants de l'histoire d'Internet.\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "final_result = overall_chain.run(\"A long article about the history of the internet...\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eK-x6ql9_tU0",
        "outputId": "8362ac08-42b4-4873-f758-20c9f5699a67"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Un article détaillé retrace l'évolution et les événements marquants de l'histoire d'Internet.\n"
          ]
        }
      ],
      "source": [
        "print(final_result)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0_2nzLRWTFS0"
      },
      "source": [
        "Generate a Story and its Title (SequentialChain):\n",
        "\n",
        "This uses SequentialChain because we have two final outputs we want to keep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "qi1YEiVmTHD6"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import SequentialChain"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "id": "QY5fBQa_THGz"
      },
      "outputs": [],
      "source": [
        "# Chain 1: Generate a story idea\n",
        "idea_prompt = PromptTemplate(\n",
        "    input_variables=[\"genre\"],\n",
        "    template=\"Generate a short story idea in the {genre} genre.\"\n",
        ")\n",
        "idea_chain = LLMChain(llm=llm, prompt=idea_prompt, output_key=\"story_idea\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 26,
      "metadata": {
        "id": "lEbNMa4yTHKN"
      },
      "outputs": [],
      "source": [
        "# Chain 2: Generate a title based on the story idea\n",
        "title_prompt = PromptTemplate(\n",
        "    input_variables=[\"story_idea\"],\n",
        "    template=\"Create a catchy title for this story idea: {story_idea}\"\n",
        ")\n",
        "title_chain = LLMChain(llm=llm, prompt=title_prompt, output_key=\"title\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "ZWhUkjM9TmOI"
      },
      "outputs": [],
      "source": [
        "# Combine them, defining input and output variables\n",
        "overall_chain = SequentialChain(\n",
        "    chains=[idea_chain, title_chain],\n",
        "    input_variables=[\"genre\"], # The initial input\n",
        "    output_variables=[\"story_idea\", \"title\"], # The final outputs we want\n",
        "    verbose=True\n",
        ")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8TxGyTohAhdn",
        "outputId": "9e112db2-49ce-408a-bb65-02382b538437"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new SequentialChain chain...\u001b[0m\n",
            "\n",
            "\u001b[1m> Finished chain.\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "result = overall_chain.invoke({\"genre\": \"sci-fi\"})\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "feEIfHTOAoRW",
        "outputId": "b9485f93-9e18-4c09-fa16-bfc837233460"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Idea: **Title: \"Echoes of Titan\"**\n",
            "\n",
            "**Synopsis:** In the year 2147, humanity has established a thriving underwater colony on Titan, Saturn's largest moon, where researchers have discovered an ancient alien artifact that emits mysterious signals. The story follows Dr. Elara Voss, a brilliant xenobiologist, who is leading a team to decode these signals, believing they hold the key to unlocking advanced technologies and insights into pre-human civilizations.\n",
            "\n",
            "As Elara and her team delve deeper into the artifact's secrets, they begin to experience strange phenomena: vivid dreams of a lush, green world, cryptic messages in their minds, and glimpses of a civilization that thrived eons ago. As the lines between reality and memory blur, the team uncovers that the artifact is not just a relic—it is a sentient device that connects to the consciousness of living beings.\n",
            "\n",
            "When corporate interests threaten to exploit the artifact for profit, Elara must rally her team to protect it, uncovering its true purpose along the way. The stakes are raised as a rival group infiltrates the colony, vying for control of the technology. Ultimately, Elara discovers that the artifact has the power to awaken the lost memories of humanity itself, reshaping their understanding of their place in the universe.\n",
            "\n",
            "In a climactic showdown, Elara confronts the choice between using the artifact to explore the minds of the ancients or shutting it down to prevent it from falling into the wrong hands. As she weighs the consequences, she learns that some truths may be too powerful to bear, yet possibly the only way to truly unite humanity for the challenges ahead. \n",
            "\n",
            "The story blends themes of identity, ethical exploration of technology, and the interconnectedness of all sentient life.\n"
          ]
        }
      ],
      "source": [
        "print(f\"Idea: {result['story_idea']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4a7W02SOAqv5",
        "outputId": "a76461c5-0a41-4f40-ba21-436102f7b95d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Title: **Title: \"Divine Signals: The Titan Awakening\"**\n"
          ]
        }
      ],
      "source": [
        "print(f\"Title: {result['title']}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PZQdE0mdUQUb"
      },
      "source": [
        "**RouterChain**\n",
        "\n",
        "A RouterChain is an advanced chain that decides which of several specialized sub-chains to use based on the input. It's like a \"switchboard\" for your chains.\n",
        "\n",
        "Structure: Input -> RouterChain -> (Chooses best sub-chain) -> Sub-chain processes input -> Output"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "id": "CFrIPc8uAz9Q"
      },
      "outputs": [],
      "source": [
        "from langchain.chains import LLMRouterChain, RouterChain\n",
        "from langchain.chains.router import MultiPromptChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain_openai import ChatOpenAI"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "QWBSmZDdAz5i"
      },
      "outputs": [],
      "source": [
        "# 1. Initialize your LLM\n",
        "llm = ChatOpenAI(model=\"gpt-3.5-turbo\", temperature=0)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 35,
      "metadata": {
        "id": "g1pCFrylAz2V"
      },
      "outputs": [],
      "source": [
        "# 2. Define destination prompts (each acts like a \"sub-chain\")\n",
        "math_prompt = PromptTemplate(\n",
        "    template=\"You are a math assistant. Solve this problem: {input}\",\n",
        "    input_variables=[\"input\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 36,
      "metadata": {
        "id": "iQ6okHlPAzzI"
      },
      "outputs": [],
      "source": [
        "history_prompt = PromptTemplate(\n",
        "    template=\"You are a historian. Answer this question: {input}\",\n",
        "    input_variables=[\"input\"],\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 37,
      "metadata": {
        "id": "NHBOGLjeAzwJ"
      },
      "outputs": [],
      "source": [
        "# 3. Put them into a dictionary\n",
        "destination_chains = {\n",
        "    \"math\": math_prompt | llm,\n",
        "    \"history\": history_prompt | llm,\n",
        "}"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "id": "35evdQIJBNh6"
      },
      "outputs": [],
      "source": [
        "# 4. Define a default chain (if no route matches)\n",
        "default_chain = llm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "tmLipiOABNew"
      },
      "outputs": [],
      "source": [
        "# 2. Prepare prompt_infos with required fields\n",
        "prompt_infos = [\n",
        "    {\n",
        "        \"name\": \"math\",\n",
        "        \"description\": \"Good for answering math questions\",\n",
        "        \"prompt_template\": \"You are a math expert. Solve: {input}\",\n",
        "    },\n",
        "    {\n",
        "        \"name\": \"history\",\n",
        "        \"description\": \"Good for answering history questions\",\n",
        "        \"prompt_template\": \"You are a historian. Answer: {input}\",\n",
        "    },\n",
        "]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JqAehLyHBNSU",
        "outputId": "b8edce33-61d6-47e3-c5b2-851a1911f79f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "\n",
            "\u001b[1m> Entering new MultiPromptChain chain...\u001b[0m\n",
            "math: {'input': 'What is 12 × 8?'}\n",
            "\u001b[1m> Finished chain.\u001b[0m\n",
            "12 × 8 = 96\n"
          ]
        }
      ],
      "source": [
        "# 3. Create the MultiPromptChain\n",
        "chain = MultiPromptChain.from_prompts(\n",
        "    llm=llm,\n",
        "    prompt_infos=prompt_infos,\n",
        "    default_chain=None,\n",
        "    verbose=True,\n",
        ")\n",
        "\n",
        "# 4. Run it\n",
        "result = chain.run(\"What is 12 × 8?\")\n",
        "print(result)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5jAddx7QZQa1"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
